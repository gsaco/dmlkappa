Metadata-Version: 2.4
Name: dmlkappa
Version: 0.1.0
Summary: Finite-Sample Conditioning Diagnostics for Double Machine Learning
Author: Gabriel Saco
License: MIT
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=1.21
Requires-Dist: pandas>=1.3
Requires-Dist: scikit-learn>=1.0
Requires-Dist: scipy>=1.7
Requires-Dist: matplotlib>=3.3
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: black; extra == "dev"
Dynamic: license-file

# dmlkappa

**Finite-Sample Conditioning Diagnostics for Double Machine Learning**

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

---

## Overview

Double Machine Learning (DML) combines flexible machine learning estimators with Neyman-orthogonal scores to obtain √n-consistent and asymptotically normal estimates of causal parameters—even when nuisance functions are estimated at slower rates. However, **finite-sample performance can vary dramatically** depending on how well-conditioned the estimation problem is.

This package implements **κ_DML**, the *condition number* for DML in the Partially Linear Regression (PLR) model:

$$
\kappa_{\mathrm{DML}} = \frac{n}{\sum_{i=1}^{n} \hat{U}_i^2}
$$

where $\hat{U}_i = D_i - \hat{m}(X_i)$ are the residualized treatments from cross-fitting.

When κ_DML is small (≈ 1), standard DML inference is reliable. When κ_DML is large, finite-sample errors are amplified—confidence intervals may undercover, and point estimates become unstable. This package provides tools to **diagnose**, **visualize**, and **interpret** finite-sample conditioning in DML applications.

---

## Key Features

- **`DMLKappaPLR`**: A scikit-learn–compatible DML estimator for PLR with built-in κ_DML diagnostics
- **Condition number computation**: Fast calculation of κ_DML from residualized treatments
- **Regime classification**: Automatic labeling as *well-conditioned*, *moderately ill-conditioned*, or *severely ill-conditioned*
- **Simulation helpers**: Generate PLR data with controlled overlap for Monte Carlo studies
- **Interpretable summaries**: Human-readable reports linking κ_DML to inferential reliability

---

## Installation

### From source (recommended for development)

```bash
git clone https://github.com/gsaco/dmlkappa.git
cd dmlkappa
pip install -e .
```

### Dependencies

- Python ≥ 3.8
- numpy
- pandas
- scikit-learn
- scipy
- matplotlib

---

## Quick Start

```python
from dmlkappa import simulate_plr, DMLKappaPLR
from sklearn.ensemble import RandomForestRegressor

# 1. Simulate PLR data with moderate overlap
X, D, Y, info = simulate_plr(
    n=1000,
    p=10,
    rho=0.5,
    overlap="moderate",
    theta0=1.0,
    random_state=42
)

# 2. Specify ML learners for nuisance functions
learner_m = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)
learner_g = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)

# 3. Fit DML-PLR with κ_DML diagnostics
model = DMLKappaPLR(
    learner_m=learner_m,
    learner_g=learner_g,
    n_splits=5,
    random_state=42
)
model.fit(X, D, Y)

# 4. Inspect results
print(model.summary())
```

**Example output:**

```
============================================================
DML-PLR Estimation Results with κ_DML Diagnostics
============================================================
  θ̂ (treatment effect)    : 1.012345
  Standard error          : 0.045678
  95% Confidence interval : [0.922816, 1.101874]
------------------------------------------------------------
Finite-Sample Conditioning Diagnostics
------------------------------------------------------------
  κ_DML                   : 1.0512
  Regime                  : moderately ill conditioned
  Effective sample size   : 475.3
  Var(Û)                  : 0.951234
============================================================
```

You can also access individual attributes:

```python
print(f"θ̂ = {model.theta_hat_:.4f}")
print(f"SE = {model.se_:.4f}")
print(f"95% CI: {model.ci_(alpha=0.05)}")
print(f"κ_DML = {model.kappa_:.2f}")
print(f"Regime: {model.regime_label()}")
```

---

## Theory Background

### The Partially Linear Regression Model

We consider the PLR model:

$$
Y = D\theta_0 + g_0(X) + \varepsilon, \quad \mathbb{E}[\varepsilon \mid D, X] = 0
$$

where θ₀ is the causal parameter of interest, g₀(X) is an unknown function, and D is the treatment.

### DML Estimation

The DML estimator uses the Neyman-orthogonal score:

$$
\psi(W; \theta, \eta) = (D - m(X))\bigl(Y - g(X) - \theta(D - m(X))\bigr)
$$

where η = (g, m) are nuisance functions estimated via cross-fitting. The estimator is:

$$
\hat{\theta} = \frac{\sum_i \hat{U}_i \hat{V}_i}{\sum_i \hat{U}_i^2}, \quad \hat{U}_i = D_i - \hat{m}(X_i), \quad \hat{V}_i = Y_i - \hat{g}(X_i)
$$

### The Condition Number κ_DML

The empirical Jacobian of the score is $\hat{J}_\theta = -\frac{1}{n}\sum_i \hat{U}_i^2$, and the condition number is:

$$
\kappa_{\mathrm{DML}} = \frac{1}{|\hat{J}_\theta|} = \frac{n}{\sum_i \hat{U}_i^2}
$$

A refined linearization shows:

$$
\hat{\theta} - \theta_0 = \kappa_{\mathrm{DML}} \cdot \{S_n + B_n\} + R_n
$$

where $S_n = O_P(1/\sqrt{n})$ is the score contribution, $B_n = o_P(1/\sqrt{n})$ is the orthogonality bias, and $R_n$ is a higher-order remainder.

**Key insight**: When κ_DML is large, the finite-sample error |θ̂ − θ₀| scales as $O_P(\kappa_{\mathrm{DML}} / \sqrt{n})$, which can be much larger than the nominal $O_P(1/\sqrt{n})$ rate.

### Conditioning Regimes

| κ_DML | Regime | Interpretation |
|-------|--------|----------------|
| < 1 | Well-conditioned | Standard DML inference reliable |
| 1–3 | Moderately ill-conditioned | Exercise caution; consider robustness checks |
| ≥ 3 | Severely ill-conditioned | Inference may be unreliable; improve overlap or use alternatives |

---

## Reproducing the Paper's Simulations

The notebook `notebooks/replicate_plr_simulations.ipynb` replicates the Monte Carlo experiments from the short communication. To run:

```bash
cd dmlkappa
pip install -e .
jupyter notebook notebooks/replicate_plr_simulations.ipynb
```

The simulation grid varies sample size (n), covariate correlation (ρ), and overlap level to demonstrate how κ_DML affects coverage and RMSE.

---

## Citation

If you use this package in your research, please cite:

```bibtex
@article{saco202X_dmlkappa,
  title   = {Finite-Sample Conditioning in Double Machine Learning: A Short Communication},
  author  = {Saco, Gabriel},
  journal = {Working Paper},
  year    = {202X},
  note    = {Preprint available upon request}
}
```

---

## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

---

## Contributing

Contributions are welcome! Please:

1. Open an issue to discuss proposed changes
2. Fork the repository and create a feature branch
3. Submit a pull request with clear description and tests

For questions or feedback, please open an issue on GitHub.
